{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    b = np.max(x)\n",
    "    numerator = np.exp(x-b)\n",
    "    probs = numerator/np.sum(numerator,axis=1,keepdims=True)\n",
    "    return probs #Normalized probabilities of each class\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self,dimensions):\n",
    "        self.weights = [0]*(len(dimensions)-1)\n",
    "        self.biases = [0]*(len(dimensions)-1)\n",
    "        for i in range(len(dimensions)-1):\n",
    "            self.weights[i] = np.random.uniform(low=(-1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                high=(1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                size=((dimensions[i],dimensions[i+1]))\n",
    "                                               )\n",
    "            self.biases[i] = np.zeros((dimensions[i+1]))   \n",
    "        self.D = dimensions[len(dimensions)-1]\n",
    "        \n",
    "    def loss(self,probs,labels):\n",
    "        N = probs.shape[0]\n",
    "        loss = -np.sum((np.log(probs[np.arange(N),labels])))\n",
    "        loss /= N\n",
    "        dprobs = np.zeros_like(probs)\n",
    "        return loss\n",
    "    \n",
    "    def fprop(self,inp,labels):\n",
    "        N = inp.shape[0]\n",
    "        #determine product of all dimensions\n",
    "        D = np.prod(inp.shape[1:])\n",
    "        #reshape inputs to Number of examples x product of all dimensions\n",
    "        data = inp.reshape((N,D))\n",
    "        activations1 = np.dot(data,self.weights[0]) + self.biases[0]\n",
    "        hidden1 = np.maximum(0,activations1)\n",
    "        \n",
    "        activations2 = np.dot(hidden1,self.weights[1]) + self.biases[1]\n",
    "        \n",
    "        probs = softmax(activations2)\n",
    "        \n",
    "        loss = self.loss(probs,labels)\n",
    "        cache = (inp,activations1,hidden1,activations2,probs,labels)\n",
    "        return loss, cache\n",
    "\n",
    "    def bprop(self,cache):\n",
    "        inp, activations1, hidden1, activations2, probs,labels = cache\n",
    "        N = inp.shape[0]\n",
    "        #grads of softmax function\n",
    "        grad_oa = probs\n",
    "        grad_oa[np.arange(labels.shape[0]),labels] -= 1\n",
    "        #grads of layer 2\n",
    "        grad_W2 = np.dot(np.transpose(hidden1),grad_oa)/N\n",
    "        grad_b2 = np.sum(grad_oa,axis=0)/N\n",
    "        grad_hs = np.dot(grad_oa,np.transpose(self.weights[1]))\n",
    "\n",
    "        #Gradient through Relu nonlinearity\n",
    "        grad_ha = grad_hs*(np.where(activations1>0,1,0))\n",
    "\n",
    "        #grads of input layer\n",
    "        grad_W1 = np.dot(np.transpose(inp),grad_ha)/N\n",
    "        grad_b1 = np.sum(grad_ha,axis=0)/N\n",
    "        grad_inp = np.dot(grad_ha,np.transpose(self.weights[0]))/N\n",
    "\n",
    "        return (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa)\n",
    "        \n",
    "    def grad_check(self,inp,labels,epsilon):\n",
    "        \n",
    "        actual_loss, cache = self.fprop(inp,labels)\n",
    "        \n",
    "        (_,grad_W1, grad_b1, _,_,grad_W2,grad_b2,_) = self.bprop(cache)\n",
    "        #Gradient check on b2\n",
    "        for i in range(self.biases[1].shape[0]):\n",
    "            self.biases[1][i] += epsilon\n",
    "            loss_perturbed_b2,_ = self.fprop(inp,labels)\n",
    "            self.biases[1][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b2-actual_loss)/epsilon\n",
    "            if grad_b2[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b2[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b2[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[1].shape[0]):\n",
    "            for j in range(self.weights[1].shape[1]):\n",
    "                self.weights[1][i,j] += epsilon\n",
    "                loss_perturbed_W2,_ = self.fprop(inp,labels)\n",
    "                self.weights[1][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W2-actual_loss)/epsilon\n",
    "                if grad_W2[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W2[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W2[i,j]\n",
    "\n",
    "        #Gradient check on b1\n",
    "        for i in range(self.biases[0].shape[0]):\n",
    "            self.biases[0][i] += epsilon\n",
    "            loss_perturbed_b1,_ = self.fprop(inp,labels)\n",
    "            self.biases[0][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b1-actual_loss)/epsilon\n",
    "            if grad_b1[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b1[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b1[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[0].shape[0]):\n",
    "            for j in range(self.weights[0].shape[1]):\n",
    "                self.weights[0][i,j] += epsilon\n",
    "                loss_perturbed_W1,_ = self.fprop(inp,labels)\n",
    "                self.weights[0][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W1-actual_loss)/epsilon\n",
    "                if grad_W1[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W1[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W1[i,j]\n",
    "                    \n",
    "    def train(self,data,learning_rate,batch_size,num_epochs):\n",
    "        num_steps = int(float(data.shape[0]/float(batch_size)))\n",
    "        for epoch in range(num_epochs):\n",
    "            for step in range(num_steps):\n",
    "                lower_bound = (step*batch_size)%data.shape[0]\n",
    "                upper_bound =(((step+1)*batch_size)%data.shape[0])\n",
    "                if upper_bound < lower_bound:\n",
    "                    upper_bound = data.shape[0]\n",
    "                features = data[lower_bound:upper_bound,:-1]\n",
    "                labels = data[lower_bound:upper_bound,-1].astype(np.int)\n",
    "                print features.shape\n",
    "                print labels.shape\n",
    "                loss, cache = self.fprop(features,labels)\n",
    "                (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa) = self.bprop(cache)\n",
    "\n",
    "                #Apply gradient descent\n",
    "                self.weights[0] -= learning_rate*grad_W1\n",
    "                self.biases[0] -= learning_rate*grad_b1\n",
    "                self.weights[1] -= learning_rate*grad_W2\n",
    "                self.biases[1] -= learning_rate*grad_b2\n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "np.random.seed(123)            \n",
    "NN = NeuralNet([10,40,2])\n",
    "example = np.random.uniform(size=(10,10))#[1,2,3,4,5,6,7,8,9,10]\n",
    "labels = np.ones((10,),dtype=np.int)\n",
    "loss,(inp,activations1,hidden1,activations2,probs,labels) = NN.fprop(example,labels)\n",
    "\n",
    "grads = NN.bprop((inp,activations1,hidden1,activations2,probs,labels))\n",
    "#NN.grad_check(example,labels,0.00001,grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotify(labels,num_classes):\n",
    "    labels = labels.astype(int)\n",
    "    onehots = np.zeros((labels.shape[0],num_classes))\n",
    "    onehots[np.arange(labels.shape[0]),labels] = 1\n",
    "    return onehots\n",
    "\n",
    "class NeuralNet_single:\n",
    "    def __init__(self,dimensions, num_classes,weight_decay=[0,0]):\n",
    "        self.weights = [0]*(len(dimensions)-1)\n",
    "        self.biases = [0]*(len(dimensions)-1)\n",
    "        for i in range(len(dimensions)-1):\n",
    "            self.weights[i] = np.random.uniform(low=(-1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                high=(1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                size=((dimensions[i+1],dimensions[i]))\n",
    "                                               )\n",
    "            self.biases[i] = np.zeros((dimensions[i+1]))   \n",
    "        self.num_classes = num_classes\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        \n",
    "    def loss(self,logit,label):\n",
    "        loss = -np.log(logit[np.argmax(label)])\n",
    "        return loss\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        b = np.max(x)\n",
    "        numerator = np.exp(x-b)\n",
    "        return numerator/np.sum(numerator)\n",
    "        \n",
    "    def fprop(self,inp,labels,train=True):\n",
    "        #determine product of all dimensions\n",
    "        D = np.prod(inp.shape[:])\n",
    "        #reshape inputs to Number of examples x product of all dimensions\n",
    "        data = inp.reshape((D))\n",
    "        activations1 = np.dot(self.weights[0],data) + self.biases[0]\n",
    "        hidden1 = np.maximum(0,activations1)\n",
    "\n",
    "        activations2 = np.dot(self.weights[1],hidden1) + self.biases[1]\n",
    "        \n",
    "        probs = self.softmax(activations2)\n",
    "        if (not train):\n",
    "            return probs\n",
    "\n",
    "        loss = self.loss(probs,labels)\n",
    "        loss += self.weight_decay[0]*(np.sum(np.abs(self.weights[0])) + np.sum(np.abs(self.weights[1]))) + \\\n",
    "                self.weight_decay[1]*(np.sum(np.square(self.weights[0]))+ np.sum(np.square(self.weights[1])))\n",
    "        \n",
    "        cache = (inp,activations1,hidden1,activations2,probs,labels)\n",
    "        \n",
    "        return loss, cache\n",
    "        \n",
    "    def bprop(self,cache):\n",
    "        inp, activation1, hidden1, activation2, probs,label = cache\n",
    "        #grads of softmax function\n",
    "        grad_oa = probs\n",
    "        grad_oa[np.argmax(label)] -= 1\n",
    "        #grads of layer 2\n",
    "        grad_W2 = np.outer(grad_oa,hidden1)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_hs = np.dot(np.transpose(self.weights[1]),grad_oa)\n",
    "\n",
    "        #Gradient through Relu nonlinearity\n",
    "        grad_ha = grad_hs*(np.where(activation1>0,1,0))\n",
    "\n",
    "        #grads of input layer\n",
    "        grad_W1 = np.outer(grad_ha,inp)\n",
    "        grad_b1 = grad_ha\n",
    "        grad_inp = np.dot(np.transpose(self.weights[0]),grad_ha)\n",
    "\n",
    "        return (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa)\n",
    "        \n",
    "    def train(self,data, learning_rate, batch_size):\n",
    "        features = data[:,:-1]\n",
    "        labels = data[:,-1]\n",
    "        onehotlabels = onehotify(labels,self.num_classes)\n",
    "        \n",
    "        for i in range(data.shape[0]/batch_size):\n",
    "            loss = 0\n",
    "            grad_inp = grad_W1 = grad_b1 = grad_ha = grad_hs = grad_W2 = grad_b2 = grad_oa = 0\n",
    "            for j in range(batch_size):\n",
    "                sample_loss, cache = self.fprop(features[(i*batch_size)+j,:],onehotlabels[(i*batch_size)+j,:])\n",
    "                loss += sample_loss\n",
    "                (sample_grad_inp, sample_grad_W1, sample_grad_b1, sample_grad_ha,\n",
    "                 sample_grad_hs,sample_grad_W2, sample_grad_b2, sample_grad_oa) = self.bprop(cache)\n",
    "                grad_W1 += sample_grad_W1\n",
    "                grad_b1 += sample_grad_b1\n",
    "                grad_W2 += sample_grad_W2\n",
    "                grad_b2 += sample_grad_b2\n",
    "                \n",
    "            loss /= batch_size\n",
    "            grad_W1 /= batch_size\n",
    "            grad_b1 /= batch_size\n",
    "            grad_W2 /= batch_size\n",
    "            grad_b2 /= batch_size\n",
    "            \n",
    "            #apply regularization\n",
    "            grad_W1 += self.weight_decay[0]*np.sign(self.weights[0]) + self.weight_decay[1]*(2*self.weights[0])\n",
    "            grad_W2 += self.weight_decay[0]*np.sign(self.weights[1]) + self.weight_decay[1]*(2*self.weights[1])\n",
    "\n",
    "            #gradient descent\n",
    "            self.weights[0] -= learning_rate*grad_W1\n",
    "            self.biases[0] -= learning_rate*grad_b1\n",
    "            self.weights[1] -= learning_rate*grad_W2\n",
    "            self.biases[1] -= learning_rate*grad_b2\n",
    "                \n",
    "            print 'Batch loss: ' + repr(loss)\n",
    "            \n",
    "    def grad_check(self,data,epsilon, batch_size = 1):\n",
    "        inp = data[:batch_size,:-1]\n",
    "        labels = data[:batch_size,-1]\n",
    "        print inp.shape\n",
    "        onehotlabels = onehotify(labels,self.num_classes)\n",
    "\n",
    "        actual_loss = grad_W1 = grad_b1 = grad_W2 = grad_b2 = 0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            sample_actual_loss,(cache) = self.fprop(inp[i,:],onehotlabels[i,:])\n",
    "            (_,sample_grad_W1, sample_grad_b1, _,_,sample_grad_W2,sample_grad_b2,_) = self.bprop(cache)\n",
    "            actual_loss += sample_actual_loss\n",
    "            grad_W1 += sample_grad_W1\n",
    "            grad_b1 += sample_grad_b1\n",
    "            grad_W2 += sample_grad_W2\n",
    "            grad_b2 += sample_grad_b2\n",
    "        actual_loss /= batch_size\n",
    "        grad_W1 /= batch_size\n",
    "        grad_b1 /= batch_size\n",
    "        grad_W2 /= batch_size\n",
    "        grad_b2 /= batch_size\n",
    "\n",
    "        #apply regularization\n",
    "        grad_W1 += self.weight_decay[0]*np.sign(self.weights[0]) + self.weight_decay[1]*(2*self.weights[0])\n",
    "        grad_W2 += self.weight_decay[0]*np.sign(self.weights[1]) + self.weight_decay[1]*(2*self.weights[1])\n",
    "        \n",
    "        #Gradient check on b2\n",
    "        print 'Second Bias gradients'\n",
    "        for i in range(self.biases[1].shape[0]):\n",
    "            self.biases[1][i] += epsilon\n",
    "            loss_perturbed_b2 = 0\n",
    "            for j in range(batch_size):\n",
    "                sample_loss_perturbed_b2,_ = self.fprop(inp[j,:],onehotlabels[j,:])\n",
    "                loss_perturbed_b2 += sample_loss_perturbed_b2\n",
    "            loss_perturbed_b2 /= batch_size\n",
    "            self.biases[1][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b2-actual_loss)/epsilon\n",
    "            if grad_b2[i] == 0:\n",
    "                print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_b2[i]+epsilon))\n",
    "            else:\n",
    "                print 'Ratio of grads: ' + repr(check_grad/grad_b2[i])\n",
    "            print 'Gradient b2[' + repr(i) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_b2[i])\n",
    "                \n",
    "        #Gradient check on W2 weights\n",
    "        print 'Second weight gradients'\n",
    "        for i in range(self.weights[1].shape[0]):\n",
    "            for j in range(self.weights[1].shape[1]):\n",
    "                self.weights[1][i,j] += epsilon\n",
    "                loss_perturbed_W2 = 0\n",
    "                for k in range(batch_size):\n",
    "                    sample_loss_perturbed_W2,_ = self.fprop(inp[k,:],onehotlabels[k,:])\n",
    "                    loss_perturbed_W2 += sample_loss_perturbed_W2\n",
    "                loss_perturbed_W2 /= batch_size\n",
    "                self.weights[1][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W2-actual_loss)/epsilon\n",
    "                if grad_W2[i,j] == 0:\n",
    "                    print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_W2[i,j]+epsilon))\n",
    "                else:\n",
    "                    print 'Ratio of grads: ' + repr(check_grad/grad_W2[i,j])\n",
    "                print 'Gradient W2[' + repr(i) + ',' + repr(j) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_W2[i,j])\n",
    "\n",
    "        #Gradient check on b1\n",
    "        print 'First bias gradients'\n",
    "        for i in range(self.biases[0].shape[0]):\n",
    "            self.biases[0][i] += epsilon\n",
    "            loss_perturbed_b1 = 0\n",
    "            for j in range(batch_size):\n",
    "                sample_loss_perturbed_b1,_ = self.fprop(inp[j,:],onehotlabels[j,:])\n",
    "                loss_perturbed_b1 += sample_loss_perturbed_b1\n",
    "            loss_perturbed_b1 /= batch_size\n",
    "            self.biases[0][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b1-actual_loss)/epsilon\n",
    "            if grad_b1[i] == 0:\n",
    "                print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_b1[i]+epsilon))\n",
    "            else:\n",
    "                print 'Ratio of grads: ' + repr(check_grad/grad_b1[i])\n",
    "            print 'Gradient b1[' + repr(i) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_b1[i])\n",
    "        #Gradient check on W1 weights\n",
    "        print 'First weights gradients'\n",
    "        for i in range(self.weights[0].shape[0]):\n",
    "            for j in range(self.weights[0].shape[1]):\n",
    "                self.weights[0][i,j] += epsilon\n",
    "                loss_perturbed_W1 = 0\n",
    "                for k in range(batch_size):\n",
    "                    sample_loss_perturbed_W1,_ = self.fprop(inp[k,:],onehotlabels[k,:])\n",
    "                    loss_perturbed_W1 += sample_loss_perturbed_W1\n",
    "                loss_perturbed_W1 /= batch_size\n",
    "                self.weights[0][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W1-actual_loss)/epsilon\n",
    "                if grad_W1[i,j] == 0:\n",
    "                    print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_W1[i,j]+epsilon))\n",
    "                else:\n",
    "                    print 'Ratio of grads: ' + repr((check_grad/grad_W1[i,j]))\n",
    "                print 'Gradient W1[' + repr(i) + ',' + repr(j) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_W1[i,j])\n",
    "        \n",
    "    \n",
    "    def eval_accuracy(self,data):\n",
    "        features = data[:,:-1]\n",
    "        labels = data[:,-1]\n",
    "        onehotlabels = onehotify(labels,self.num_classes)\n",
    "        N = features.shape[0]\n",
    "        probs = np.zeros_like(onehotlabels)\n",
    "        for i in range(N):\n",
    "            probs[i,:] = self.fprop(features[i,:],onehotlabels[i,:],train=False)\n",
    "        predictions = np.argmax(probs,axis=1)\n",
    "        accuracy = np.mean(np.equal(predictions,labels))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleNN = NeuralNet_single([2,2,2],2,[.1,.1])\n",
    "#singleNN.train(data,0.1,25)\n",
    "#accuracy = singleNN.eval_accuracy(data[0:10])\n",
    "#print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set Up: 2 Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"twomoons.txt\"\n",
    "data = np.loadtxt(open(fname,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1,2: Gradient Check on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "Second Bias gradients\n",
      "Ratio of grads: 1.0000171036707608\n",
      "Gradient b2[0]: Finite difference: 0.072697865061144284 Analytical: 0.072696621682061605\n",
      "Ratio of grads: 0.99998289682755503\n",
      "Gradient b2[1]: Finite difference: -0.072695378339204808 Analytical: -0.072696621682061605\n",
      "Second weight gradients\n",
      "Ratio of grads: 1.0000065542196164\n",
      "Gradient W2[0,0]: Finite difference: 1.5473188032188998 Analytical: 1.547308661818116\n",
      "Ratio of grads: 0.99999154685252423\n",
      "Gradient W2[0,1]: Finite difference: -1.1877927470926863 Analytical: -1.1878027877648234\n",
      "Ratio of grads: 1.0000041677924205\n",
      "Gradient W2[1,0]: Finite difference: 2.4332599659615539 Analytical: 2.4332498246813774\n",
      "Ratio of grads: 1.0000067247838755\n",
      "Gradient W2[1,1]: Finite difference: 1.4931025006603702 Analytical: 1.4930924599362709\n",
      "First bias gradients\n",
      "Ratio of grads: 1.0000015226790806\n",
      "Gradient b1[0]: Finite difference: 0.044353379546890885 Analytical: 0.04435331201103053\n",
      "Ratio of grads: 0.99999791921443792\n",
      "Gradient b1[1]: Finite difference: -0.032788194914701307 Analytical: -0.032788263140045855\n",
      "First weights gradients\n",
      "Ratio of grads: 1.0000064486471001\n",
      "Gradient W1[0,0]: Finite difference: 1.5528571290168711 Analytical: 1.5528471152538244\n",
      "Ratio of grads: 0.99999400334139266\n",
      "Gradient W1[0,1]: Finite difference: -1.6972320323382915 Analytical: -1.6972422101203994\n",
      "Ratio of grads: 0.99999427943899122\n",
      "Gradient W1[1,0]: Finite difference: -1.7557195063133688 Analytical: -1.755729550071375\n",
      "Ratio of grads: 1.0000093388459641\n",
      "Gradient W1[1,1]: Finite difference: 1.0834453750163675 Analytical: 1.0834352569813905\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "singleNN = NeuralNet_single([2,2,2],2, [1,1])\n",
    "#take one example\n",
    "# reshape data from single column vectore to single row vector\n",
    "singleNN.grad_check(data,0.00001,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments 3,4: Gradient Check on minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-0e5e6964d025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msingleNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-998e99aba323>\u001b[0m in \u001b[0;36mgrad_check\u001b[0;34m(self, data, epsilon, batch_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "singleNN.grad_check(data,0.00001,[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
