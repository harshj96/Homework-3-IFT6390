{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00000200884\n",
      "0.99999799116\n",
      "1.0000002994\n",
      "0.999999700496\n",
      "1.0\n",
      "1.0\n",
      "1.00000051201\n",
      "0.999999487985\n",
      "1.00000026844\n",
      "0.999999730603\n",
      "1.00000037358\n",
      "0.999999626179\n",
      "1.00000084182\n",
      "0.999999158177\n",
      "1.00000052921\n",
      "0.999999471092\n",
      "1.00000083655\n",
      "0.99999916356\n",
      "1.00000043595\n",
      "0.999999563907\n",
      "1.00000063228\n",
      "0.999999367699\n",
      "1.00000074644\n",
      "0.999999253575\n",
      "1.00000117035\n",
      "0.999998829628\n",
      "1.00000018949\n",
      "0.999999811187\n",
      "1.0000013271\n",
      "0.999998672917\n",
      "1.00000042655\n",
      "0.999999573296\n",
      "1.00000052499\n",
      "0.999999474747\n",
      "1.0\n",
      "1.0\n",
      "1.00000033545\n",
      "0.999999665037\n",
      "1.00000014052\n",
      "0.999999860393\n",
      "1.0\n",
      "1.0\n",
      "1.00000024523\n",
      "0.999999754926\n",
      "1.00000111103\n",
      "0.999998888993\n",
      "1.00000029617\n",
      "0.999999704236\n",
      "1.00000051043\n",
      "0.999999489418\n",
      "1.00000033369\n",
      "0.99999966647\n",
      "1.0\n",
      "1.0\n",
      "1.00000020883\n",
      "0.999999790744\n",
      "1.0\n",
      "1.0\n",
      "1.00000079121\n",
      "0.99999920882\n",
      "1.00000037368\n",
      "0.999999624894\n",
      "1.0\n",
      "1.0\n",
      "1.00000054104\n",
      "0.999999459064\n",
      "1.0\n",
      "1.0\n",
      "1.00000042557\n",
      "0.999999574406\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000080488\n",
      "0.999999195082\n",
      "1.00000014834\n",
      "0.999999851058\n",
      "1.0000004858\n",
      "0.999999514116\n",
      "1.0000001056\n",
      "1.0\n",
      "1.00000055687\n",
      "1.00000012919\n",
      "1.00000025815\n",
      "0.9999998148\n",
      "1.00000020167\n",
      "0.99999984604\n",
      "1.00000002094\n",
      "0.999999679415\n",
      "1.0000001349\n",
      "1.00000008523\n",
      "0.999999575977\n",
      "1.00000036768\n",
      "1.00000015526\n",
      "1.00000010627\n",
      "1.0\n",
      "1.00000013681\n",
      "1.00000054836\n",
      "1.0\n",
      "1.0000000312\n",
      "1.00000037329\n",
      "0.999999449687\n",
      "1.00000037245\n",
      "1.00000025642\n",
      "1.0\n",
      "0.999999835\n",
      "1.0\n",
      "1.00000000332\n",
      "0.999999845854\n",
      "1.0\n",
      "0.999999797269\n",
      "1.0\n",
      "1.00000040797\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000059293\n",
      "1.00000036722\n",
      "1.00000033665\n",
      "1.00000006808\n",
      "1.0\n",
      "1.00000027485\n",
      "1.00000007804\n",
      "1.00000015285\n",
      "0.999999897525\n",
      "1.00000012964\n",
      "0.999999915195\n",
      "1.0000000093\n",
      "0.999999821955\n",
      "1.00000007418\n",
      "1.00000004791\n",
      "0.99999982374\n",
      "1.00000020452\n",
      "1.00000007539\n",
      "1.00000007024\n",
      "1.0\n",
      "1.00000008025\n",
      "1.00000011403\n",
      "1.0\n",
      "1.00000000561\n",
      "1.0000002077\n",
      "0.999999864805\n",
      "1.00000018814\n",
      "1.00000015287\n",
      "1.0\n",
      "0.999999914003\n",
      "1.0\n",
      "1.0000000305\n",
      "1.00000001138\n",
      "1.0\n",
      "0.999999888868\n",
      "1.0\n",
      "1.00000023193\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000034664\n",
      "1.00000018688\n",
      "1.00000018701\n",
      "1.00000006909\n",
      "1.0\n",
      "1.00000039678\n",
      "1.00000010734\n",
      "1.00000020011\n",
      "0.999999867052\n",
      "1.00000015202\n",
      "0.999999889549\n",
      "1.0000000158\n",
      "0.999999764828\n",
      "1.00000009689\n",
      "1.00000006119\n",
      "0.999999712501\n",
      "1.00000026436\n",
      "1.00000005912\n",
      "1.00000008146\n",
      "1.0\n",
      "1.0000001136\n",
      "1.00000045967\n",
      "1.0\n",
      "1.00000001775\n",
      "1.00000026864\n",
      "0.999999712511\n",
      "1.00000022749\n",
      "1.00000019445\n",
      "1.0\n",
      "0.999999916274\n",
      "1.0\n",
      "1.00000001479\n",
      "0.999999896886\n",
      "1.0\n",
      "0.999999865266\n",
      "1.0\n",
      "1.00000030329\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000038476\n",
      "1.00000010051\n",
      "1.0000002421\n",
      "1.00000007391\n",
      "1.0\n",
      "1.00000043144\n",
      "1.00000009609\n",
      "1.00000020234\n",
      "0.999999856151\n",
      "1.00000017775\n",
      "0.999999880715\n",
      "1.00000001359\n",
      "0.999999745321\n",
      "1.00000010385\n",
      "1.00000006445\n",
      "0.999999706341\n",
      "1.00000028493\n",
      "1.00000010236\n",
      "1.00000009253\n",
      "1.0\n",
      "1.00000010444\n",
      "1.00000031903\n",
      "1.0\n",
      "1.00000002107\n",
      "1.00000028947\n",
      "0.9999997053\n",
      "1.00000027281\n",
      "1.00000019665\n",
      "1.0\n",
      "0.999999867717\n",
      "1.0\n",
      "1.00000000333\n",
      "0.999999888561\n",
      "1.0\n",
      "0.999999846105\n",
      "1.0\n",
      "1.00000032372\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0000004741\n",
      "1.00000015646\n",
      "1.00000026096\n",
      "1.00000008324\n",
      "1.0\n",
      "1.00000036777\n",
      "1.00000008079\n",
      "1.00000015117\n",
      "0.99999987942\n",
      "1.00000013727\n",
      "0.999999899861\n",
      "1.00000001751\n",
      "0.999999797158\n",
      "1.00000008685\n",
      "1.00000005455\n",
      "0.999999702215\n",
      "1.00000023903\n",
      "1.00000011404\n",
      "1.00000005338\n",
      "1.0\n",
      "1.0000000946\n",
      "1.00000034834\n",
      "1.0\n",
      "1.00000000088\n",
      "1.00000024315\n",
      "0.999999605959\n",
      "1.00000025717\n",
      "1.00000016987\n",
      "1.0\n",
      "0.999999887828\n",
      "1.0\n",
      "1.0000000176\n",
      "0.999999898378\n",
      "1.0\n",
      "0.999999864825\n",
      "1.0\n",
      "1.00000023152\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000038752\n",
      "1.00000032157\n",
      "1.00000021862\n",
      "1.00000003648\n",
      "1.0\n",
      "1.00000037922\n",
      "0.999999989248\n",
      "1.00000014852\n",
      "0.99999988151\n",
      "1.00000007249\n",
      "0.999999901179\n",
      "1.00000001133\n",
      "0.99999979133\n",
      "1.00000008577\n",
      "1.00000005091\n",
      "0.999999703195\n",
      "1.00000023506\n",
      "1.00000011348\n",
      "1.00000002871\n",
      "1.0\n",
      "1.0000000672\n",
      "1.00000030892\n",
      "1.0\n",
      "1.00000001901\n",
      "1.00000023871\n",
      "0.999999802652\n",
      "1.00000025316\n",
      "1.00000014735\n",
      "1.0\n",
      "0.999999980188\n",
      "1.0\n",
      "1.00000003544\n",
      "1.00000004465\n",
      "1.0\n",
      "0.999999871431\n",
      "1.0\n",
      "1.00000027155\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000038673\n",
      "1.00000013433\n",
      "1.0000002156\n",
      "1.00000008367\n",
      "1.0\n",
      "1.00000037217\n",
      "1.00000011629\n",
      "1.00000017133\n",
      "0.999999874122\n",
      "1.00000015698\n",
      "0.999999895864\n",
      "1.00000002176\n",
      "0.999999788209\n",
      "1.00000009106\n",
      "1.00000005713\n",
      "0.999999735274\n",
      "1.0000002495\n",
      "1.00000011341\n",
      "1.00000007856\n",
      "1.0\n",
      "1.00000009658\n",
      "1.00000035483\n",
      "1.0\n",
      "1.00000000126\n",
      "1.00000025344\n",
      "0.999999561118\n",
      "1.00000024752\n",
      "1.00000018625\n",
      "1.0\n",
      "0.999999908004\n",
      "1.0\n",
      "1.00000001807\n",
      "0.999999962526\n",
      "1.0\n",
      "0.999999873229\n",
      "1.0\n",
      "1.0000002636\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000043388\n",
      "1.00000031244\n",
      "1.00000022827\n",
      "1.00000005097\n",
      "1.0\n",
      "1.00000032547\n",
      "1.00000009956\n",
      "1.00000014261\n",
      "0.999999897519\n",
      "1.0000001305\n",
      "0.999999915058\n",
      "1.00000000711\n",
      "0.999999817732\n",
      "1.00000007367\n",
      "1.00000004762\n",
      "0.999999779967\n",
      "1.00000020352\n",
      "1.00000008066\n",
      "1.00000006264\n",
      "1.0\n",
      "1.00000008191\n",
      "1.00000028554\n",
      "1.0\n",
      "1.0000000078\n",
      "1.00000020691\n",
      "0.99999965587\n",
      "1.0000001794\n",
      "1.00000014966\n",
      "1.0\n",
      "0.999999943731\n",
      "1.0\n",
      "1.00000002468\n",
      "0.999999939213\n",
      "1.0\n",
      "0.999999904957\n",
      "1.0\n",
      "1.0000002193\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000034897\n",
      "1.00000020391\n",
      "1.00000018652\n",
      "1.0000000867\n",
      "1.0\n",
      "1.00000034712\n",
      "1.00000009758\n",
      "1.00000017032\n",
      "0.999999876092\n",
      "1.00000016819\n",
      "0.999999897493\n",
      "1.00000002096\n",
      "0.999999780291\n",
      "1.00000008959\n",
      "1.00000005597\n",
      "0.999999759334\n",
      "1.00000024567\n",
      "1.00000009101\n",
      "1.00000007951\n",
      "1.0\n",
      "1.00000010568\n",
      "1.00000030249\n",
      "1.0\n",
      "0.999999999814\n",
      "1.00000024959\n",
      "1.00000000146\n",
      "1.00000020875\n",
      "1.00000018671\n",
      "1.0\n",
      "0.999999934839\n",
      "1.0\n",
      "1.00000000051\n",
      "0.999999967233\n",
      "1.0\n",
      "0.99999987501\n",
      "1.0\n",
      "1.0000002575\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000042481\n",
      "1.00000027045\n",
      "1.00000022516\n",
      "1.00000006893\n",
      "1.0\n",
      "1.00000043022\n",
      "1.00000004978\n",
      "1.00000017374\n",
      "0.999999864648\n",
      "1.00000013891\n",
      "0.999999887685\n",
      "1.00000001211\n",
      "0.999999759278\n",
      "1.00000009778\n",
      "1.00000006216\n",
      "0.99999966292\n",
      "1.00000026885\n",
      "1.00000011829\n",
      "1.00000005688\n",
      "1.0\n",
      "1.00000009747\n",
      "1.00000035916\n",
      "1.0\n",
      "1.00000002205\n",
      "1.0000002732\n",
      "0.999999700005\n",
      "1.00000028398\n",
      "1.00000017324\n",
      "1.0\n",
      "0.999999878463\n",
      "1.0\n",
      "1.00000001436\n",
      "0.999999879371\n",
      "1.0\n",
      "0.999999848137\n",
      "1.0\n",
      "1.00000029166\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000042368\n",
      "1.00000026115\n",
      "1.0000002464\n",
      "1.00000007603\n",
      "1.0\n",
      "1.00000038408\n",
      "1.00000007513\n",
      "1.00000015721\n",
      "0.999999878429\n",
      "1.00000014555\n",
      "0.999999898257\n",
      "1.00000001356\n",
      "0.999999784555\n",
      "1.00000008851\n",
      "1.00000005574\n",
      "0.999999700764\n",
      "1.00000024123\n",
      "1.00000011109\n",
      "1.00000005663\n",
      "1.0\n",
      "1.00000009585\n",
      "1.00000042335\n",
      "1.0\n",
      "1.000000008\n",
      "1.00000024474\n",
      "0.999999526448\n",
      "1.00000025684\n",
      "1.00000017801\n",
      "1.0\n",
      "0.999999937093\n",
      "1.0\n",
      "1.00000000146\n",
      "1.00000001666\n",
      "1.0\n",
      "0.99999986937\n",
      "1.0\n",
      "1.00000024423\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.00000040214\n",
      "1.00000029853\n",
      "1.00000022112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    b = np.max(x)\n",
    "    numerator = np.exp(x-b)\n",
    "    probs = numerator/np.sum(numerator,axis=1,keepdims=True)\n",
    "    return probs #Normalized probabilities of each class\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self,dimensions):\n",
    "        self.weights = [0]*(len(dimensions)-1)\n",
    "        self.biases = [0]*(len(dimensions)-1)\n",
    "        for i in range(len(dimensions)-1):\n",
    "            self.weights[i] = np.random.uniform(low=(-1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                high=(1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                size=((dimensions[i],dimensions[i+1]))\n",
    "                                               )\n",
    "            self.biases[i] = np.zeros((dimensions[i+1]))   \n",
    "        self.D = dimensions[len(dimensions)-1]\n",
    "        \n",
    "    def loss(self,probs,labels):\n",
    "        N = probs.shape[0]\n",
    "        loss = -np.sum((np.log(probs[np.arange(N),labels])))\n",
    "        loss /= N\n",
    "        dprobs = np.zeros_like(probs)\n",
    "        return loss\n",
    "    \n",
    "    def fprop(self,inp,labels):\n",
    "        N = inp.shape[0]\n",
    "        #determine product of all dimensions\n",
    "        D = np.prod(inp.shape[1:])\n",
    "        #reshape inputs to Number of examples x product of all dimensions\n",
    "        data = inp.reshape((N,D))\n",
    "        activations1 = np.dot(data,self.weights[0]) + self.biases[0]\n",
    "        hidden1 = np.maximum(0,activations1)\n",
    "        \n",
    "        activations2 = np.dot(hidden1,self.weights[1]) + self.biases[1]\n",
    "        \n",
    "        probs = softmax(activations2)\n",
    "        \n",
    "        loss = self.loss(probs,labels)\n",
    "        cache = (inp,activations1,hidden1,activations2,probs,labels)\n",
    "        return loss, cache\n",
    "\n",
    "    def bprop(self,cache):\n",
    "        inp, activations1, hidden1, activations2, probs,labels = cache\n",
    "        N = inp.shape[0]\n",
    "        #grads of softmax function\n",
    "        grad_oa = probs\n",
    "        grad_oa[np.arange(labels.shape[0]),labels] -= 1\n",
    "        #grads of layer 2\n",
    "        grad_W2 = np.dot(np.transpose(hidden1),grad_oa)/N\n",
    "        grad_b2 = np.sum(grad_oa,axis=0)/N\n",
    "        grad_hs = np.dot(grad_oa,np.transpose(self.weights[1]))\n",
    "\n",
    "        #Gradient through Relu nonlinearity\n",
    "        grad_ha = grad_hs*(np.where(activations1>0,1,0))\n",
    "\n",
    "        #grads of input layer\n",
    "        grad_W1 = np.dot(np.transpose(inp),grad_ha)/N\n",
    "        grad_b1 = np.sum(grad_ha,axis=0)/N\n",
    "        grad_inp = np.dot(grad_ha,np.transpose(self.weights[0]))/N\n",
    "\n",
    "        return (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa)\n",
    "        \n",
    "    def grad_check(self,inp,labels,epsilon,cache):\n",
    "        (_,grad_W1,grad_b1,_,_,grad_W2,grad_b2,_) = cache\n",
    "        actual_loss, _ = self.fprop(inp,labels)\n",
    "        #Gradient check on b2\n",
    "        for i in range(self.biases[1].shape[0]):\n",
    "            self.biases[1][i] += epsilon\n",
    "            loss_perturbed_b2,_ = self.fprop(inp,labels)\n",
    "            self.biases[1][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b2-actual_loss)/epsilon\n",
    "            if grad_b2[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b2[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b2[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[1].shape[0]):\n",
    "            for j in range(self.weights[1].shape[1]):\n",
    "                self.weights[1][i,j] += epsilon\n",
    "                loss_perturbed_W2,_ = self.fprop(inp,labels)\n",
    "                self.weights[1][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W2-actual_loss)/epsilon\n",
    "                if grad_W2[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W2[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W2[i,j]\n",
    "\n",
    "        #Gradient check on b1\n",
    "        for i in range(self.biases[0].shape[0]):\n",
    "            self.biases[0][i] += epsilon\n",
    "            loss_perturbed_b1,_ = self.fprop(inp,labels)\n",
    "            self.biases[0][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b1-actual_loss)/epsilon\n",
    "            if grad_b1[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b1[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b1[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[0].shape[0]):\n",
    "            for j in range(self.weights[0].shape[1]):\n",
    "                self.weights[0][i,j] += epsilon\n",
    "                loss_perturbed_W1,_ = self.fprop(inp,labels)\n",
    "                self.weights[0][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W1-actual_loss)/epsilon\n",
    "                if grad_W1[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W1[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W1[i,j]\n",
    "                    \n",
    "    def train(self,data,learning_rate,batch_size,num_epochs):\n",
    "        num_steps = int(float(data.shape[0]/float(batch_size)))\n",
    "        for epoch in range(num_epochs):\n",
    "            for step in range(num_steps):\n",
    "                lower_bound = (step*batch_size)%data.shape[0]\n",
    "                upper_bound =(((step+1)*batch_size)%data.shape[0])\n",
    "                if upper_bound < lower_bound:\n",
    "                    upper_bound = data.shape[0]\n",
    "                features = data[lower_bound:upper_bound,:-1]\n",
    "                labels = data[lower_bound:upper_bound,-1].astype(np.int)\n",
    "                print features.shape\n",
    "                print labels.shape\n",
    "                loss, cache = self.fprop(features,labels)\n",
    "                (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa) = self.bprop(cache)\n",
    "\n",
    "                #Apply gradient descent\n",
    "                self.weights[0] -= learning_rate*grad_W1\n",
    "                self.biases[0] -= learning_rate*grad_b1\n",
    "                self.weights[1] -= learning_rate*grad_W2\n",
    "                self.biases[1] -= learning_rate*grad_b2\n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "np.random.seed(123)            \n",
    "NN = NeuralNet([10,40,2])\n",
    "example = np.random.uniform(size=(10,10))#[1,2,3,4,5,6,7,8,9,10]\n",
    "labels = np.ones((10,),dtype=np.int)\n",
    "loss,(inp,activations1,hidden1,activations2,probs,labels) = NN.fprop(example,labels)\n",
    "\n",
    "grads = NN.bprop((inp,activations1,hidden1,activations2,probs,labels))\n",
    "NN.grad_check(example,labels,0.00001,grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"twomoons.txt\"\n",
    "data = np.loadtxt(open(fname,'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100,)\n",
      "<type 'numpy.int64'>\n",
      "(1100, 2)\n",
      "[[ 0.36540813 -1.7957237   0.        ]\n",
      " [-1.2084724   0.39429077  1.        ]\n",
      " [ 0.38618144  1.0483956   0.        ]\n",
      " [-0.63657005 -1.6521227   0.        ]\n",
      " [-0.80009178 -0.71212892  1.        ]]\n",
      "<type 'numpy.float64'>\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n",
      "(5, 2)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "NN_moon = NeuralNet([2,10,2])\n",
    "examples = data[:,:-1]\n",
    "labels = data[:,-1]\n",
    "labels = labels.astype(np.int)\n",
    "print labels.shape\n",
    "print type(labels[-1])\n",
    "print examples.shape\n",
    "#data = np.column_stack((examples,labels))\n",
    "print data[0:5,:]\n",
    "print type(data[0,-1])\n",
    "NN_moon.train(data,0.001,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
