{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    b = np.max(x)\n",
    "    numerator = np.exp(x-b)\n",
    "    probs = numerator/np.sum(numerator,axis=1,keepdims=True)\n",
    "    return probs #Normalized probabilities of each class\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self,dimensions):\n",
    "        self.weights = [0]*(len(dimensions)-1)\n",
    "        self.biases = [0]*(len(dimensions)-1)\n",
    "        for i in range(len(dimensions)-1):\n",
    "            self.weights[i] = np.random.uniform(low=(-1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                high=(1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                size=((dimensions[i],dimensions[i+1]))\n",
    "                                               )\n",
    "            self.biases[i] = np.zeros((dimensions[i+1]))   \n",
    "        self.D = dimensions[len(dimensions)-1]\n",
    "        \n",
    "    def loss(self,probs,labels):\n",
    "        N = probs.shape[0]\n",
    "        loss = -np.sum((np.log(probs[np.arange(N),labels])))\n",
    "        loss /= N\n",
    "        dprobs = np.zeros_like(probs)\n",
    "        return loss\n",
    "    \n",
    "    def fprop(self,inp,labels):\n",
    "        N = inp.shape[0]\n",
    "        #determine product of all dimensions\n",
    "        D = np.prod(inp.shape[1:])\n",
    "        #reshape inputs to Number of examples x product of all dimensions\n",
    "        data = inp.reshape((N,D))\n",
    "        activations1 = np.dot(data,self.weights[0]) + self.biases[0]\n",
    "        hidden1 = np.maximum(0,activations1)\n",
    "        \n",
    "        activations2 = np.dot(hidden1,self.weights[1]) + self.biases[1]\n",
    "        \n",
    "        probs = softmax(activations2)\n",
    "        \n",
    "        loss = self.loss(probs,labels)\n",
    "        cache = (inp,activations1,hidden1,activations2,probs,labels)\n",
    "        return loss, cache\n",
    "\n",
    "    def bprop(self,cache):\n",
    "        inp, activations1, hidden1, activations2, probs,labels = cache\n",
    "        N = inp.shape[0]\n",
    "        #grads of softmax function\n",
    "        grad_oa = probs\n",
    "        grad_oa[np.arange(labels.shape[0]),labels] -= 1\n",
    "        #grads of layer 2\n",
    "        grad_W2 = np.dot(np.transpose(hidden1),grad_oa)/N\n",
    "        grad_b2 = np.sum(grad_oa,axis=0)/N\n",
    "        grad_hs = np.dot(grad_oa,np.transpose(self.weights[1]))\n",
    "\n",
    "        #Gradient through Relu nonlinearity\n",
    "        grad_ha = grad_hs*(np.where(activations1>0,1,0))\n",
    "\n",
    "        #grads of input layer\n",
    "        grad_W1 = np.dot(np.transpose(inp),grad_ha)/N\n",
    "        grad_b1 = np.sum(grad_ha,axis=0)/N\n",
    "        grad_inp = np.dot(grad_ha,np.transpose(self.weights[0]))/N\n",
    "\n",
    "        return (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa)\n",
    "        \n",
    "    def grad_check(self,inp,labels,epsilon):\n",
    "        \n",
    "        actual_loss, cache = self.fprop(inp,labels)\n",
    "        \n",
    "        (_,grad_W1, grad_b1, _,_,grad_W2,grad_b2,_) = self.bprop(cache)\n",
    "        #Gradient check on b2\n",
    "        for i in range(self.biases[1].shape[0]):\n",
    "            self.biases[1][i] += epsilon\n",
    "            loss_perturbed_b2,_ = self.fprop(inp,labels)\n",
    "            self.biases[1][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b2-actual_loss)/epsilon\n",
    "            if grad_b2[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b2[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b2[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[1].shape[0]):\n",
    "            for j in range(self.weights[1].shape[1]):\n",
    "                self.weights[1][i,j] += epsilon\n",
    "                loss_perturbed_W2,_ = self.fprop(inp,labels)\n",
    "                self.weights[1][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W2-actual_loss)/epsilon\n",
    "                if grad_W2[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W2[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W2[i,j]\n",
    "\n",
    "        #Gradient check on b1\n",
    "        for i in range(self.biases[0].shape[0]):\n",
    "            self.biases[0][i] += epsilon\n",
    "            loss_perturbed_b1,_ = self.fprop(inp,labels)\n",
    "            self.biases[0][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b1-actual_loss)/epsilon\n",
    "            if grad_b1[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b1[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b1[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[0].shape[0]):\n",
    "            for j in range(self.weights[0].shape[1]):\n",
    "                self.weights[0][i,j] += epsilon\n",
    "                loss_perturbed_W1,_ = self.fprop(inp,labels)\n",
    "                self.weights[0][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W1-actual_loss)/epsilon\n",
    "                if grad_W1[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W1[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W1[i,j]\n",
    "                    \n",
    "    def train(self,data,learning_rate,batch_size,num_epochs):\n",
    "        num_steps = int(float(data.shape[0]/float(batch_size)))\n",
    "        for epoch in range(num_epochs):\n",
    "            for step in range(num_steps):\n",
    "                lower_bound = (step*batch_size)%data.shape[0]\n",
    "                upper_bound =(((step+1)*batch_size)%data.shape[0])\n",
    "                if upper_bound < lower_bound:\n",
    "                    upper_bound = data.shape[0]\n",
    "                features = data[lower_bound:upper_bound,:-1]\n",
    "                labels = data[lower_bound:upper_bound,-1].astype(np.int)\n",
    "                print features.shape\n",
    "                print labels.shape\n",
    "                loss, cache = self.fprop(features,labels)\n",
    "                (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa) = self.bprop(cache)\n",
    "\n",
    "                #Apply gradient descent\n",
    "                self.weights[0] -= learning_rate*grad_W1\n",
    "                self.biases[0] -= learning_rate*grad_b1\n",
    "                self.weights[1] -= learning_rate*grad_W2\n",
    "                self.biases[1] -= learning_rate*grad_b2\n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "np.random.seed(123)            \n",
    "NN = NeuralNet([10,40,2])\n",
    "example = np.random.uniform(size=(10,10))#[1,2,3,4,5,6,7,8,9,10]\n",
    "labels = np.ones((10,),dtype=np.int)\n",
    "loss,(inp,activations1,hidden1,activations2,probs,labels) = NN.fprop(example,labels)\n",
    "\n",
    "grads = NN.bprop((inp,activations1,hidden1,activations2,probs,labels))\n",
    "#NN.grad_check(example,labels,0.00001,grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotify(labels,num_classes):\n",
    "    labels = labels.astype(int)\n",
    "    onehots = np.zeros((labels.shape[0],num_classes))\n",
    "    onehots[np.arange(labels.shape[0]),labels] = 1\n",
    "    return onehots\n",
    "\n",
    "class NeuralNet_single:\n",
    "    def __init__(self,dimensions):\n",
    "        self.weights = [0]*(len(dimensions)-1)\n",
    "        self.biases = [0]*(len(dimensions)-1)\n",
    "        for i in range(len(dimensions)-1):\n",
    "            self.weights[i] = np.random.uniform(low=(-1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                high=(1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                size=((dimensions[i+1],dimensions[i]))\n",
    "                                               )\n",
    "            self.biases[i] = np.zeros((dimensions[i+1]))   \n",
    "        self.D = dimensions[len(dimensions)-1]\n",
    "    def loss(self,logit,label):\n",
    "        loss = -np.log(logit[np.argmax(label)])\n",
    "        return loss\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        b = np.max(x)\n",
    "        numerator = np.exp(x-b)\n",
    "        return numerator/np.sum(numerator)\n",
    "        \n",
    "    def fprop(self,inp,labels,train=True):\n",
    "        #determine product of all dimensions\n",
    "        D = np.prod(inp.shape[:])\n",
    "        #reshape inputs to Number of examples x product of all dimensions\n",
    "        data = inp.reshape((D))\n",
    "        activations1 = np.dot(self.weights[0],data) + self.biases[0]\n",
    "        hidden1 = np.maximum(0,activations1)\n",
    "\n",
    "        activations2 = np.dot(self.weights[1],hidden1) + self.biases[1]\n",
    "        \n",
    "        probs = self.softmax(activations2)\n",
    "        if (not train):\n",
    "            return probs\n",
    "\n",
    "        loss = self.loss(probs,labels)\n",
    "        cache = (inp,activations1,hidden1,activations2,probs,labels)\n",
    "        \n",
    "        return loss, cache\n",
    "        \n",
    "    def bprop(self,cache):\n",
    "        inp, activation1, hidden1, activation2, probs,label = cache\n",
    "        #grads of softmax function\n",
    "        grad_oa = probs\n",
    "        grad_oa[np.argmax(label)] -= 1\n",
    "        #grads of layer 2\n",
    "        grad_W2 = np.outer(grad_oa,hidden1)\n",
    "        grad_b2 = grad_oa\n",
    "        grad_hs = np.dot(np.transpose(self.weights[1]),grad_oa)\n",
    "\n",
    "        #Gradient through Relu nonlinearity\n",
    "        grad_ha = grad_hs*(np.where(activation1>0,1,0))\n",
    "\n",
    "        #grads of input layer\n",
    "        grad_W1 = np.outer(grad_ha,inp)\n",
    "        grad_b1 = grad_ha\n",
    "        grad_inp = np.dot(np.transpose(self.weights[0]),grad_ha)\n",
    "\n",
    "        return (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa)\n",
    "        \n",
    "    def train(self,data, learning_rate,num_classes, batch_size):\n",
    "        features = data[:,:-1]\n",
    "        labels = data[:,-1]\n",
    "        onehotlabels = onehotify(labels,num_classes)\n",
    "        \n",
    "        for i in range(data.shape[0]/batch_size):\n",
    "            loss = 0\n",
    "            grad_inp = grad_W1 = grad_b1 = grad_ha = grad_hs = grad_W2 = grad_b2 = grad_oa = 0\n",
    "            for j in range(batch_size):\n",
    "                sample_loss, cache = self.fprop(features[(i*batch_size)+j,:],onehotlabels[(i*batch_size)+j,:])\n",
    "                loss += sample_loss\n",
    "                (sample_grad_inp, sample_grad_W1, sample_grad_b1, sample_grad_ha,\n",
    "                 sample_grad_hs,sample_grad_W2, sample_grad_b2, sample_grad_oa) = self.bprop(cache)\n",
    "                grad_W1 += sample_grad_W1\n",
    "                grad_b1 += sample_grad_b1\n",
    "                grad_W2 += sample_grad_W2\n",
    "                grad_b2 += sample_grad_b2\n",
    "                \n",
    "            loss /= batch_size\n",
    "            grad_W1 /= batch_size\n",
    "            grad_b1 /= batch_size\n",
    "            grad_W2 /= batch_size\n",
    "            grad_b2 /= batch_size\n",
    "            #gradient descent\n",
    "            self.weights[0] -= learning_rate*grad_W1\n",
    "            self.biases[0] -= learning_rate*grad_b1\n",
    "            self.weights[1] -= learning_rate*grad_W2\n",
    "            self.biases[1] -= learning_rate*grad_b2\n",
    "                \n",
    "            print 'Batch loss: ' + repr(loss)\n",
    "            \n",
    "    def grad_check(self,data,epsilon,num_classes, batch_size = 1):\n",
    "        \n",
    "        inp = data[:batch_size,:-1]\n",
    "        labels = data[:batch_size,-1]\n",
    "        print inp.shape\n",
    "        onehotlabels = onehotify(labels,num_classes)\n",
    "        actual_loss = grad_W1 = grad_b1 = grad_W2 = grad_b2 = 0\n",
    "        for i in range(batch_size):\n",
    "            sample_actual_loss,(cache) = self.fprop(inp[i,:],onehotlabels[i,:])\n",
    "            (_,sample_grad_W1, sample_grad_b1, _,_,sample_grad_W2,sample_grad_b2,_) = self.bprop(cache)\n",
    "            actual_loss += sample_actual_loss\n",
    "            grad_W1 += sample_grad_W1\n",
    "            grad_b1 += sample_grad_b1\n",
    "            grad_W2 += sample_grad_W2\n",
    "            grad_b2 += sample_grad_b2\n",
    "        actual_loss /= batch_size\n",
    "        grad_W1 /= batch_size\n",
    "        grad_b1 /= batch_size\n",
    "        grad_W2 /= batch_size\n",
    "        grad_b2 /= batch_size\n",
    "\n",
    "        \n",
    "        #Gradient check on b2\n",
    "        print 'Second Bias gradients'\n",
    "        for i in range(self.biases[1].shape[0]):\n",
    "            self.biases[1][i] += epsilon\n",
    "            loss_perturbed_b2 = 0\n",
    "            for j in range(batch_size):\n",
    "                sample_loss_perturbed_b2,_ = self.fprop(inp[j,:],onehotlabels[j,:])\n",
    "                loss_perturbed_b2 += sample_loss_perturbed_b2\n",
    "            loss_perturbed_b2 /= batch_size\n",
    "            self.biases[1][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b2-actual_loss)/epsilon\n",
    "            if grad_b2[i] == 0:\n",
    "                print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_b2[i]+epsilon))\n",
    "            else:\n",
    "                print 'Ratio of grads: ' + repr(check_grad/grad_b2[i])\n",
    "            print 'Gradient b2[' + repr(i) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_b2[i])\n",
    "                \n",
    "        #Gradient check on W2 weights\n",
    "        print 'Second weight gradients'\n",
    "        for i in range(self.weights[1].shape[0]):\n",
    "            for j in range(self.weights[1].shape[1]):\n",
    "                self.weights[1][i,j] += epsilon\n",
    "                loss_perturbed_W2 = 0\n",
    "                for k in range(batch_size):\n",
    "                    sample_loss_perturbed_W2,_ = self.fprop(inp[k,:],onehotlabels[k,:])\n",
    "                    loss_perturbed_W2 += sample_loss_perturbed_W2\n",
    "                loss_perturbed_W2 /= batch_size\n",
    "                self.weights[1][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W2-actual_loss)/epsilon\n",
    "                if grad_W2[i,j] == 0:\n",
    "                    print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_W2[i,j]+epsilon))\n",
    "                else:\n",
    "                    print 'Ratio of grads: ' + repr(check_grad/grad_W2[i,j])\n",
    "                print 'Gradient W2[' + repr(i) + ',' + repr(j) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_W2[i,j])\n",
    "\n",
    "        #Gradient check on b1\n",
    "        print 'First bias gradients'\n",
    "        for i in range(self.biases[0].shape[0]):\n",
    "            self.biases[0][i] += epsilon\n",
    "            loss_perturbed_b1 = 0\n",
    "            for j in range(batch_size):\n",
    "                sample_loss_perturbed_b1,_ = self.fprop(inp[j,:],onehotlabels[j,:])\n",
    "                loss_perturbed_b1 += sample_loss_perturbed_b1\n",
    "            loss_perturbed_b1 /= batch_size\n",
    "            self.biases[0][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b1-actual_loss)/epsilon\n",
    "            if grad_b1[i] == 0:\n",
    "                print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_b1[i]+epsilon))\n",
    "            else:\n",
    "                print 'Ratio of grads: ' + repr(check_grad/grad_b1[i])\n",
    "            print 'Gradient b1[' + repr(i) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_b1[i])\n",
    "        #Gradient check on W1 weights\n",
    "        print 'First weights gradients'\n",
    "        for i in range(self.weights[0].shape[0]):\n",
    "            for j in range(self.weights[0].shape[1]):\n",
    "                self.weights[0][i,j] += epsilon\n",
    "                loss_perturbed_W1 = 0\n",
    "                for k in range(batch_size):\n",
    "                    sample_loss_perturbed_W1,_ = self.fprop(inp[k,:],onehotlabels[k,:])\n",
    "                    loss_perturbed_W1 += sample_loss_perturbed_W1\n",
    "                loss_perturbed_W1 /= batch_size\n",
    "                self.weights[0][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W1-actual_loss)/epsilon\n",
    "                if grad_W1[i,j] == 0:\n",
    "                    print 'Ratio of grads: ' + repr((check_grad+epsilon)/(grad_W1[i,j]+epsilon))\n",
    "                else:\n",
    "                    print 'Ratio of grads: ' + repr((check_grad/grad_W1[i,j]))\n",
    "                print 'Gradient W1[' + repr(i) + ',' + repr(j) + ']: Finite difference: ' + repr(check_grad) + ' Analytical: ' + repr(grad_W1[i,j])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleNN = NeuralNet_single([2,2,2])\n",
    "\n",
    "features = np.random.uniform(size=(1,2))\n",
    "labels = np.ones((1,1))\n",
    "\n",
    "data = np.hstack((features,labels))\n",
    "\n",
    "#singleNN.grad_check(data,0.00001,2)\n",
    "#singleNN.train(data,0.01,2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set Up: 2 Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"twomoons.txt\"\n",
    "data = np.loadtxt(open(fname,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1,2: Gradient Check on a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "Second Bias gradients\n",
      "Ratio of grads: 0.99999754404916141\n",
      "Gradient b2[0]: Finite difference: -0.5088096762251304 Analytical: -0.50881092583975041\n",
      "Ratio of grads: 1.0000024559604943\n",
      "Gradient b2[1]: Finite difference: 0.50881217545928337 Analytical: 0.50881092583975041\n",
      "Second weight gradients\n",
      "Ratio of grads: 0.99999803663086917\n",
      "Gradient W2[0,0]: Finite difference: -0.40676216724877529 Analytical: -0.40676296587462607\n",
      "Ratio of grads: 1.0\n",
      "Gradient W2[0,1]: Finite difference: 0.0 Analytical: 0.0\n",
      "Ratio of grads: 1.000001963378865\n",
      "Gradient W2[1,0]: Finite difference: 0.40676376450443635 Analytical: 0.40676296587462607\n",
      "Ratio of grads: 1.0\n",
      "Gradient W2[1,1]: Finite difference: 0.0 Analytical: 0.0\n",
      "First bias gradients\n",
      "Ratio of grads: 1.0000001084323871\n",
      "Gradient b1[0]: Finite difference: 0.022433549573541708 Analytical: 0.022433547141018643\n",
      "Ratio of grads: 1.0\n",
      "Gradient b1[1]: Finite difference: 0.0 Analytical: 0.0\n",
      "First weights gradients\n",
      "Ratio of grads: 1.0000000383949628\n",
      "Gradient W1[0,0]: Finite difference: 0.0081974008248053565 Analytical: 0.0081974005100664687\n",
      "Ratio of grads: 0.99999980537475486\n",
      "Gradient W1[0,1]: Finite difference: -0.040284444435823019 Analytical: -0.04028445227619442\n",
      "Ratio of grads: 1.0\n",
      "Gradient W1[1,0]: Finite difference: 0.0 Analytical: 0.0\n",
      "Ratio of grads: 1.0\n",
      "Gradient W1[1,1]: Finite difference: 0.0 Analytical: 0.0\n"
     ]
    }
   ],
   "source": [
    "singleNN = NeuralNet_single([2,2,2])\n",
    "#take one example\n",
    "# reshape data from single column vectore to single row vector\n",
    "singleNN.grad_check(data,0.00001,2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments 3,4: Gradient Check on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
