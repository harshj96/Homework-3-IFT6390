{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.]\n",
      "[[ 0.5735648  -0.5735648 ]\n",
      " [ 0.53267377 -0.53267377]\n",
      " [ 0.50343878 -0.50343878]\n",
      " [ 0.55478725 -0.55478725]]\n",
      "1.00000228813\n",
      "0.999997711877\n",
      "1.00000046263\n",
      "0.999999537281\n",
      "1.00000020877\n",
      "0.999999791194\n",
      "1.00000006994\n",
      "0.999999931349\n",
      "1.0\n",
      "1.0\n",
      "1.00000114454\n",
      "0.999998855496\n",
      "1.0000007361\n",
      "0.999999263785\n",
      "1.00000053027\n",
      "0.999999469577\n",
      "1.00000067481\n",
      "0.999999325194\n",
      "1.00000090064\n",
      "0.999999099291\n",
      "1.00000033379\n",
      "0.999999666165\n",
      "1.0\n",
      "1.0\n",
      "1.00000088741\n",
      "0.999999112493\n",
      "1.00000019072\n",
      "0.999999809623\n",
      "1.00000096961\n",
      "0.999999030446\n",
      "1.0000004847\n",
      "0.999999515437\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.000000401\n",
      "0.99999959881\n",
      "1.00000017878\n",
      "0.999999821473\n",
      "1.00000057208\n",
      "0.99999942778\n",
      "1.0000004869\n",
      "0.999999658949\n",
      "1.00000019528\n",
      "1.0\n",
      "1.0000006263\n",
      "1.0000002801\n",
      "0.999999621154\n",
      "0.999999928147\n",
      "0.999999857136\n",
      "1.00000067398\n",
      "1.0\n",
      "0.999999303928\n",
      "1.00000080575\n",
      "0.999999959154\n",
      "1.00000025911\n",
      "1.0\n",
      "1.0\n",
      "1.00000091719\n",
      "0.999999303039\n",
      "0.999999911176\n",
      "1.00000019026\n",
      "0.999999905324\n",
      "1.00000005437\n",
      "1.0\n",
      "1.00000024463\n",
      "1.00000010914\n",
      "0.99999992044\n",
      "0.999999961094\n",
      "0.999999943152\n",
      "1.00000026282\n",
      "1.0\n",
      "0.99999972868\n",
      "1.00000022671\n",
      "0.999999981146\n",
      "1.00000007416\n",
      "1.0\n",
      "1.0\n",
      "1.00000038648\n",
      "0.999999626217\n",
      "0.999999974407\n",
      "1.00000019868\n",
      "0.99999987679\n",
      "1.00000007228\n",
      "1.0\n",
      "1.00000025561\n",
      "1.00000011402\n",
      "0.99999983177\n",
      "0.999999972173\n",
      "0.999999941304\n",
      "1.00000027508\n",
      "1.0\n",
      "0.999999715913\n",
      "1.00000034716\n",
      "0.999999982205\n",
      "1.00000011141\n",
      "1.0\n",
      "1.0\n",
      "1.00000035304\n",
      "0.999999714454\n",
      "0.999999962561\n",
      "1.00000044952\n",
      "1.00000006328\n",
      "1.00000000698\n",
      "1.0\n",
      "1.00000057808\n",
      "1.00000025823\n",
      "0.999999729195\n",
      "0.999999928789\n",
      "0.999999868178\n",
      "1.00000062211\n",
      "1.0\n",
      "0.999999357565\n",
      "1.00000077494\n",
      "0.999999963894\n",
      "1.00000024986\n",
      "1.0\n",
      "1.0\n",
      "1.0000008084\n",
      "0.999999309463\n",
      "0.999999934827\n",
      "1.00000017348\n",
      "0.999999874535\n",
      "1.00000007161\n",
      "1.0\n",
      "1.00000022375\n",
      "1.00000010029\n",
      "0.999999844391\n",
      "0.999999973402\n",
      "0.999999948686\n",
      "1.00000024048\n",
      "1.0\n",
      "0.999999751521\n",
      "1.00000029823\n",
      "0.999999980177\n",
      "1.00000009551\n",
      "1.0\n",
      "1.0\n",
      "1.00000031539\n",
      "0.999999780288\n",
      "0.999999963199\n",
      "1.000000194\n",
      "0.999999818168\n",
      "1.00000010386\n",
      "1.0\n",
      "1.00000024896\n",
      "1.00000011139\n",
      "0.999999928039\n",
      "0.999999976223\n",
      "0.9999999425\n",
      "1.00000026788\n",
      "1.0\n",
      "0.999999723168\n",
      "1.00000038214\n",
      "0.999999983954\n",
      "1.00000012286\n",
      "1.0\n",
      "1.0\n",
      "1.00000028945\n",
      "0.999999767472\n",
      "0.999999962392\n",
      "1.00000032978\n",
      "0.99999994444\n",
      "1.00000002978\n",
      "1.0\n",
      "1.00000042424\n",
      "1.00000018959\n",
      "0.999999633833\n",
      "0.999999970438\n",
      "0.999999902902\n",
      "1.00000045644\n",
      "1.0\n",
      "0.999999528584\n",
      "1.00000030572\n",
      "0.999999971895\n",
      "1.0000000981\n",
      "1.0\n",
      "1.0\n",
      "1.0000007474\n",
      "0.99999974108\n",
      "0.999999923489\n",
      "1.00000035074\n",
      "0.999999796532\n",
      "1.00000011626\n",
      "1.0\n",
      "1.00000045129\n",
      "1.00000020194\n",
      "0.999999753305\n",
      "0.999999939113\n",
      "0.999999896795\n",
      "1.00000048574\n",
      "1.0\n",
      "0.99999949845\n",
      "1.00000065916\n",
      "0.999999973113\n",
      "1.00000021212\n",
      "1.0\n",
      "1.0\n",
      "1.00000051566\n",
      "0.999999424392\n",
      "0.999999944534\n",
      "1.00000028365\n",
      "0.999999900081\n",
      "1.00000005742\n",
      "1.0\n",
      "1.00000036489\n",
      "1.00000016332\n",
      "0.999999672347\n",
      "0.999999977476\n",
      "0.999999916277\n",
      "1.00000039276\n",
      "1.0\n",
      "0.999999594547\n",
      "1.00000025718\n",
      "0.999999979547\n",
      "1.00000008323\n",
      "1.0\n",
      "1.0\n",
      "1.00000065882\n",
      "0.999999796037\n",
      "0.999999935106\n",
      "1.00000030333\n",
      "0.999999784452\n",
      "1.00000012214\n",
      "1.0\n",
      "1.00000039021\n",
      "1.00000017467\n",
      "0.999999989567\n",
      "0.999999952505\n",
      "0.999999911013\n",
      "1.0000004199\n",
      "1.0\n",
      "0.999999566454\n",
      "1.00000058247\n",
      "0.999999975965\n",
      "1.00000018718\n",
      "1.0\n",
      "1.0\n",
      "1.0000002634\n",
      "0.999999555014\n",
      "0.99999994986\n",
      "1.00000031299\n",
      "0.999999981092\n",
      "1.00000001205\n",
      "1.0\n",
      "1.00000040206\n",
      "1.00000017972\n",
      "0.999999898696\n",
      "0.999999947598\n",
      "0.999999908428\n",
      "1.00000043294\n",
      "1.0\n",
      "0.999999552908\n",
      "1.00000061118\n",
      "0.999999973285\n",
      "1.00000019731\n",
      "1.0\n",
      "1.0\n",
      "1.00000048846\n",
      "0.999999499028\n",
      "0.999999979597\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    b = np.max(x)\n",
    "    numerator = np.exp(x-b)\n",
    "    probs = numerator/np.sum(numerator,axis=1,keepdims=True)\n",
    "    return probs #Normalized probabilities of each class\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self,dimensions):\n",
    "        self.weights = [0]*(len(dimensions)-1)\n",
    "        self.biases = [0]*(len(dimensions)-1)\n",
    "        for i in range(len(dimensions)-1):\n",
    "            self.weights[i] = np.random.uniform(low=(-1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                high=(1)/float((np.sqrt(dimensions[i]))),\n",
    "                                                size=((dimensions[i],dimensions[i+1]))\n",
    "                                               )\n",
    "            self.biases[i] = np.zeros((dimensions[i+1]))   \n",
    "        self.D = dimensions[len(dimensions)-1]\n",
    "    \n",
    "    def affine_backward(self,output_grad):\n",
    "        x = self.affine_cache[i]\n",
    "        N = x.shape[0]\n",
    "        D = np.prod(x.shape[1:])\n",
    "        inp = x.reshape((N,D))\n",
    "        \n",
    "    def loss(self,probs,labels):\n",
    "        N = probs.shape[0]\n",
    "        loss = -np.sum((np.log(probs[np.arange(N),labels])))\n",
    "        loss /= N\n",
    "        dprobs = np.zeros_like(probs)\n",
    "        #print dout\n",
    "        #dprobs[np.arange(N),labels] = -1/probs[np.arange(N),labels]\n",
    "        #dprobs /= N\n",
    "        return loss\n",
    "    \n",
    "    def fprop(self,inp,labels):\n",
    "        N = inp.shape[0]\n",
    "        #determine product of all dimensions\n",
    "        D = np.prod(inp.shape[1:])\n",
    "        #reshape inputs to Number of examples x product of all dimensions\n",
    "        data = inp.reshape((N,D))\n",
    "        activations1 = np.dot(data,self.weights[0]) + self.biases[0]\n",
    "        hidden1 = np.maximum(0,activations1)\n",
    "        \n",
    "        activations2 = np.dot(hidden1,self.weights[1]) + self.biases[1]\n",
    "        \n",
    "        probs = softmax(activations2)\n",
    "        \n",
    "        loss = self.loss(probs,labels)\n",
    "        cache = (inp,activations1,hidden1,activations2,probs,labels)\n",
    "        return loss, cache\n",
    "\n",
    "    def bprop(self,cache):\n",
    "        inp, activations1, hidden1, activations2, probs,labels = cache\n",
    "        N = inp.shape[0]\n",
    "        #grads of softmax function\n",
    "        grad_oa = probs\n",
    "        \n",
    "        print np.sum(grad_oa,axis=1)\n",
    "        grad_oa[np.arange(labels.shape[0]),labels] -= 1\n",
    "        print grad_oa\n",
    "        #grads of layer 2\n",
    "        grad_W2 = np.dot(np.transpose(hidden1),grad_oa)/N\n",
    "        grad_b2 = np.sum(grad_oa,axis=0)/N\n",
    "        grad_hs = np.dot(grad_oa,np.transpose(self.weights[1]))\n",
    "\n",
    "        #Gradient through Relu nonlinearity\n",
    "        grad_ha = grad_hs*(np.where(activations1>0,1,0))\n",
    "\n",
    "        #grads of input layer\n",
    "        grad_W1 = np.dot(np.transpose(inp),grad_ha)/N\n",
    "        grad_b1 = np.sum(grad_ha,axis=0)/N\n",
    "        grad_inp = np.dot(grad_ha,np.transpose(self.weights[0]))/N\n",
    "\n",
    "        return (grad_inp, grad_W1, grad_b1, grad_ha, grad_hs,grad_W2, grad_b2, grad_oa)\n",
    "        \n",
    "        \n",
    "    def grad_check(self,inp,labels,epsilon,cache):\n",
    "        (_,grad_W1,grad_b1,_,_,grad_W2,grad_b2,_) = cache\n",
    "        actual_loss, _ = self.fprop(inp,labels)\n",
    "        #Gradient check on b2\n",
    "        for i in range(self.biases[1].shape[0]):\n",
    "            self.biases[1][i] += epsilon\n",
    "            loss_perturbed_b2,_ = self.fprop(inp,labels)\n",
    "            self.biases[1][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b2-actual_loss)/epsilon\n",
    "            if grad_b2[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b2[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b2[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[1].shape[0]):\n",
    "            for j in range(self.weights[1].shape[1]):\n",
    "                self.weights[1][i,j] += epsilon\n",
    "                loss_perturbed_W2,_ = self.fprop(inp,labels)\n",
    "                self.weights[1][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W2-actual_loss)/epsilon\n",
    "                if grad_W2[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W2[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W2[i,j]\n",
    "\n",
    "        #Gradient check on b1\n",
    "        for i in range(self.biases[0].shape[0]):\n",
    "            self.biases[0][i] += epsilon\n",
    "            loss_perturbed_b1,_ = self.fprop(inp,labels)\n",
    "            self.biases[0][i] -= epsilon\n",
    "            check_grad = (loss_perturbed_b1-actual_loss)/epsilon\n",
    "            if grad_b1[i] == 0:\n",
    "                print (check_grad+epsilon)/(grad_b1[i]+epsilon)\n",
    "            else:\n",
    "                print check_grad/grad_b1[i]\n",
    "        #Gradient check on W2 weights\n",
    "        for i in range(self.weights[0].shape[0]):\n",
    "            for j in range(self.weights[0].shape[1]):\n",
    "                self.weights[0][i,j] += epsilon\n",
    "                loss_perturbed_W1,_ = self.fprop(inp,labels)\n",
    "                self.weights[0][i,j] -= epsilon\n",
    "                check_grad = (loss_perturbed_W1-actual_loss)/epsilon\n",
    "                if grad_W1[i,j] == 0:\n",
    "                    print (check_grad+epsilon)/(grad_W1[i,j]+epsilon)\n",
    "                else:\n",
    "                    print check_grad/grad_W1[i,j]\n",
    "        \n",
    "\n",
    "np.random.seed(123)            \n",
    "NN = NeuralNet([10,20,2])\n",
    "example = np.random.uniform(size=(4,10))#[1,2,3,4,5,6,7,8,9,10]\n",
    "labels = np.ones((4,),dtype=np.int)\n",
    "loss,(inp,activations1,hidden1,activations2,probs,labels) = NN.fprop(example,labels)\n",
    "\n",
    "grads = NN.bprop((inp,activations1,hidden1,activations2,probs,labels))\n",
    "NN.grad_check(example,labels,0.00001,grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
